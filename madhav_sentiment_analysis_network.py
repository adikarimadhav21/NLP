# -*- coding: utf-8 -*-
"""Madhav: Sentiment Analysis Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fboPfC37h5mhwdBuoUCoPerGxLcswzxr

# Necessary installs and imports
"""

# pip install datasets

import collections
import datasets

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchtext
import tqdm

"""#Get training and testing data"""

#"imdb" on one of datasets in the PyTorch datasets library
#This line loads the dataset, splits it in half, and stores each half as a "train" and "test" set

#You can find more information about PyTorch datasets, and other available datasets here:
#https://pytorch.org/text/stable/datasets.html#imdb
print(datasets.__version__)

train_data, test_data = datasets.load_dataset("imdb", split=["train", "test"])

#We can print the dataset and features to see the categories and data sizes
#Currently, we have 25,000 training samples and 25,000 testing samples
#Each sample has a "text" category (with the review)
#and a "label" category (with the review classification- 0 or 1 in this set)
print(train_data, test_data)
print(train_data.features)

#We can also print individual samples to see what the data looks like
print(train_data[0])

"""# Preprocessing input text

"""

#To create our embedding dictionary we need to get individual words and assign each one an index
#We will pass in the *index* for each word in each sample to get the vector for each word

#Use a built-in PyTorch (torchtext) command to tokenize the text
#https://pytorch.org/text/stable/data_utils.html
tokenizer = torchtext.data.utils.get_tokenizer("basic_english")

#Test the tokenizer
tokenizer("This is a test. Wow, this is so much easier that what I did originally...")

#This function takes each individual sample (movie review in our case) and splits
#them into individual tokens. It returns a dictionary of the tokens with a label.
#We can use that to add a new category to our train and test datasets
def tokenize_example(example, tokenizer):
    tokens = tokenizer(example["text"])
    return {"tokens": tokens}

#"map" is a Python command that lets us apply a function to each item of an iterable dataset
#In this chunk of code, we ask the computer to apply the "tokenize_example" function to each
#sample in our train and test datasets. This will tokenize each input, and add them as a new
#feature. "fn_kwargs" stands for "function keyword arguments". It is used to pass in additional
#arguments to map (since our function requires the tokenizer as well as the sample)
train_data = train_data.map(tokenize_example, fn_kwargs = {"tokenizer": tokenizer})
test_data  = test_data.map(tokenize_example,  fn_kwargs = {"tokenizer": tokenizer})

#We can print the dataset and features again to verify that each sample now
#contains the text input ("text"), true classification ("label"), AND the
#tokenized list ("tokens")
print(train_data)
print(train_data.features)

#Compare the added tokens to the text from the original review
#How is using the trained tokenizer different from just using .split() to get
#individual words? Why would this be helpful for NLP tasks?
print(train_data[0])

#Use train_test_split command on the training dataset to break off a portion of the
#training samples to create a validation set. In this case, we break off 25% of the
#training dataset to make a new set we can evaluate on while training.

#Even though we have to assign the label "test", this data is NOT part of our
#actual test dataset. We can use validation to evaluate effectiveness while still training.
train_valid_data = train_data.train_test_split(test_size = 0.25)
train_data = train_valid_data["train"]
valid_data = train_valid_data["test"]

#Verify that 75% of the train set is in train and 25% has been moved to a new valid set
#The test dataset has not been touched
print(len(train_data), len(valid_data), len(test_data))

#We need to assign an *index* to each token, and store them in a vocabulary dictionary
#Use a built-in PyTorch (torchtext) command to assign an index to each token
#    -part of the "vocab" module
#In this example, only words that appear at least 5 times will be added to the dictionary
#We use special tokens "unknown" to replace words that appear fewer than 5 times
#and "pad" to pad empty spaces so that our input text will have a uniform size
#https://pytorch.org/text/stable/vocab.html
vocab = torchtext.vocab.build_vocab_from_iterator(
    train_data["tokens"],
    min_freq = 5,
    specials = ["<unknown>", "<pad>"])

#See how many unique tokens are in our vocab
#Note: this number might be different for different groups/runs of the program
#because the train_test_split randomizes samples to assign for "train_data"

#"get_itos" stands for "get index to string"
#It returns a list mapping indices to their tokens
print(len(vocab))
print(vocab.get_itos()[:20])

#So, "vocab" is a *dictionary* with each token as the key and its index as the value
#We can test this by getting the index for random words in our training data set
print(vocab["and"])
print("giraffe" in vocab)
print("love" in vocab)

#Every time the model encounters our "unknown" placeholder, we will assign the default
#index of 0 (the index of that special character in our vocabulary)
vocab.set_default_index(0)

#If a word is in the vocab dictionary, we can look-up its index
#Otherwise, we get the default index of 0
#(We assigned the "pad" special character to index 1 when we made vocab)
print(vocab["giraffe"])
vocab.lookup_indices(["hello", "world", "Mika", "<pad>"])

#Remember that what we actually need is the *index* for each word, not the word itself
#We will pass in the indices to get the embedding vectors for the embedding layer.
#So, we want to replace each word in our input samples with their respective indices.

#This function is passed in each movie review (as "example") and the vocab dictionary.
#It takes the "tokens" category for each review, and returns a list with the index
#for each respective token. We label it so we can add it to our data sets as a new feature.
def numericalize_example(example, vocab):
    ids = vocab.lookup_indices(example["tokens"])
    return {"ids": ids}

#Just like we used map before to add the tokens to each sample,
#we now use map to add the list of token indices to each sample
#Again, we need to pass in another argument, because the function needs
#the vocab dictionary to be able to look-up the index for each token.
train_data = train_data.map(numericalize_example, fn_kwargs={"vocab": vocab})
valid_data = valid_data.map(numericalize_example, fn_kwargs={"vocab": vocab})
test_data  = test_data.map (numericalize_example, fn_kwargs={"vocab": vocab})

#Print out the first 10 tokens in the first sample
print(train_data[0]["tokens"][:10])

#Verify that the indices for each of these tokens are the same using the look-up table
#in the original vocab dictionary and the "ids" category we just added to the data sets.
#You might be able to see "unknown" (0), "pad" (1), or a repeated token with repeated indices.
print(vocab.lookup_indices(train_data[0]["tokens"][:10]))
print(train_data[0]["ids"][:10])

#We have finally finished preprocessing! Yay!
#Now we convert our data samples to tensors- the data structure for PyTorch
#Specify type = torch to make PyTorch tensors, specify that only the sample
#token indices ("ids") and true classifications ("labels") are needed
train_data = train_data.with_format(type = "torch", columns=["ids", "label"])
valid_data = valid_data.with_format(type = "torch", columns=["ids", "label"])
test_data  = test_data.with_format (type = "torch", columns=["ids", "label"])

"""# Load and shuffle preprocessed input text"""

#"collate" means to group things together in a proper order
#Here, we group each epoch of the dataset into batches
#Training data in batches allows us to update the model parameters more quickly
#For more information on collate_fn, see: https://pytorch.org/docs/stable/data.html
def get_collate_fn():
    def collate_fn(batch):

        #For each sample in this batch (i), gather all token indices, store in batch_ids
        batch_ids = [i["ids"] for i in batch]

        #Pad the empty spaces with the placeholder to make all samples the same length
        #(1 is index of "pad" placeholder in index dictionary)
        batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value = 1, batch_first = True)

        #For each sample in this batch (i), gather all true classifications
        #Use torch.stack to concatenate all classifications into one tensor
        batch_label = [i["label"] for i in batch]
        batch_label = torch.stack(batch_label)

        #Combine the indices and correct labels for all 512 samples; return
        batch = {"ids": batch_ids, "label": batch_label}
        return batch

    return collate_fn

#The DataLoader class is a PyTorch method that allows us to iterate over a dataset
#We use it to load data in for the model. It enables batching, shuffling, and controling order.
#For more information on DataLoader, see: https://pytorch.org/docs/stable/data.html

#Here we pass in our dataset and set a default param of shuffle = False
#(The only dataset we want to shuffle is the training data, so we will override that one to True)
def get_data_loader(dataset, shuffle = False):
    collate_fn = get_collate_fn()
    data_loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = 512,
        collate_fn = collate_fn, shuffle = shuffle)
    return data_loader

#Actually call the get_data_loader function for each of our three datasets
train_data_loader = get_data_loader(train_data, shuffle = True)
valid_data_loader = get_data_loader(valid_data)
test_data_loader  = get_data_loader(test_data)

"""# Create neural network class

"""

#Pass in "nn.Module" as the base for all PyTorch neural networks
#Our neural network class needs two methods- init and forward
#Define your layers in init, and define the calculations needed to make each
#prediction in forward
class ReviewClassifier(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()

        #Start with embedding layer for each review
        #PyTorch "Embedding" layer is used to store word embeddings and retrieve them using indices
        #Pass in optional params: size of embedding dictionary, size of each embedding vector,
        #and the index for the pad symbol (will be ignored when calculating gradient)
        #https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
        self.embedding = nn.Embedding(vocab_size, 300, padding_idx = 1)

        #Create a final output layer that accepts the embeddings as input
        #"Linear" is a PyTorch method that performs (x * w + b)
        #First param is size of input, second param is size of output
        #The input is of size 300 because our embedding vectors are of size 300 (GloVe)
        #The output is of size 2 because we have two classifications (0 or 1)
        self.output = nn.Linear(300, 2)

    def forward(self, ids):
        #We don't need an activation function for embeddings
        #Just pass in the indices of which words you are retrieving, and return as is
        embedded = self.embedding(ids)

        #Pooling is taking all embedding vectors for this sample text and aggregating into one vector
        #(Could accomplish by taking sum, mean, max, etc.)
        pooled = embedded.mean(dim = 1)

        #The output layer in the network accepts input of size 300 (dimensions on one pooled vector)
        #and returns output of the Linear transformation
        prediction = self.output(pooled)

        return prediction

#Define the input param needed for the class, and make an instance of the model
#Vocabulary size is the length of our vocab dictionary (all of the tokens/indices)
vocab_size = len(vocab)

model = ReviewClassifier(vocab_size)

"""# Get embedding vectors for input text"""

#torchtext has GloVe vector embeddings built-in
#Warning: this code block takes ~10 - 15 minutes to run
vectors = torchtext.vocab.GloVe()

#"get_vocs_by_tokens" takes a token or list of tokens as input and returns their embedding vectors
#See more about using torchtext for GloVe embeddings and retrieving vectors here:
#https://pytorch.org/text/stable/vocab.html

#We then use those embedding vectors as the weights for the embedding layer in our nn
pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())
model.embedding.weight.data = pretrained_embedding

"""# Define functions to train and evaluate model"""

#Here I used the Cross Entropy Loss cost function and
#Stochastic Gradient Descent optimizer, since we discussed them in class.
#You might try changing one (or both) of these to check for accuracy improvement

#The SGD parameters set the model.parameters as the values to take the partial
#derivatives wrt, and sets the learning rate ("lr") to 0.1
cost = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)

#Define the function to actually train the model
#This is function is used with the training dataset
def train(data_loader, model, cost, optimizer):

    #Set the model to "train" mode (calculate gradient and update parameters)
    model.train()

    #Create empty lists to store the running loss and accuracy over time
    #Use it later to visualize the model training (trend line)
    epoch_loss     = []
    epoch_accuracy = []

    #"tqdm" comes from the Arabic word "taqaddum," which means "progress"
    #It is used to show progress bars for loops/iterables (especially if they take a long time to run)
    for batch in tqdm.tqdm(data_loader):

        #Get the sample text token indices and true classifications for current batch
        ids = batch["ids"]
        label = batch["label"]

        #Use the forward pass in the model to make a prediction (have to pass in sample indices)
        prediction = model(ids)

        #Compare the model prediction with the true classification to determine loss
        loss = cost(prediction, label)
        accuracy = get_accuracy(prediction, label)

        #Don't forget to reset the computational graph between each iteration
        #(Otherwise each calculation will be cumulative with all prior results)
        optimizer.zero_grad()

        #Use the backward pass to calculate the gradient of the loss function
        loss.backward()

        #Update the model parameters (new = old - lr * gradient)
        optimizer.step()

        #Add the loss and accuracy to our lists so we can see how the model
        #improves (or doesn't) as it trains
        epoch_loss.append(loss.item())
        epoch_accuracy.append(accuracy.item())

    return np.mean(epoch_loss), np.mean(epoch_accuracy)

#Define the function to evaluate the model performance
#This function is used with the validation dataset
def evaluate(data_loader, model, cost):

    #Set the model to "eval" mode (don't calculate gradient or update parameters)
    model.eval()

    #Create empty lists to store the running loss and accuracy over time
    #Use it later to visualize the model training (trend line)
    epoch_loss     = []
    epoch_accuracy = []

    #Use this flag to not calculate the gradient during these steps
    with torch.no_grad():

        #Repeat the same logic as above to get the predictions and compare to true labels
        #Notice that you DON'T have backward, step, or 0-ing computational graph this time
        for batch in tqdm.tqdm(data_loader):
            ids = batch["ids"]
            label = batch["label"]
            prediction = model(ids)
            loss = cost(prediction, label)
            accuracy = get_accuracy(prediction, label)

            #Add the loss and accuracy to our lists so we can see how the model
            #improves (or doesn't) as it trains
            epoch_loss.append(loss.item())
            epoch_accuracy.append(accuracy.item())

    return np.mean(epoch_loss), np.mean(epoch_accuracy)

#Define the function to calculate accuracy for each batch
def get_accuracy(prediction, label):

    #Get the batch size dynamically for each input (512 for all batches EXCEPT last one)
    batch_size, _ = prediction.shape

    #"prediction" is a tensor with the probabilities for the two classes (0 and 1)
    #"argmax" gets the classification with the highest probability
    predicted_classes = prediction.argmax(dim = -1)

    #Sum up all instances of the predicted class matching the true classification (label)
    correct_predictions = predicted_classes.eq(label).sum()

    #Divide the number of correct predictions by the batch size; return result
    accuracy = correct_predictions / batch_size
    return accuracy

"""# Train model, track loss and accuracy over time"""

#Train the model

#Keep track of the best tested loss, store the parameters that resulted in the best loss
#Those are the parameters we will keep when we exit training
best_valid_loss = float("inf")

#Keep running totals of the loss and accuracy for the training and validation datasets
#Loss should go down over time, and accuracy should go up
#We want to be able to see these to ensure the model is working/training correctly
train_loss     = []
train_accuracy = []
valid_loss     = []
valid_accuracy = []

#Loop 10 epochs (entire dataset, broken up into ~35 iterations with 512 samples each)
for epoch in range(10):

    #Call the train and evaluate functions, passing in the datasets (loaded with DataLoader),
    #model (instance of nn class), cost function (CEL), and optimizer (SGD)
    #Functions return the average loss and accuracy for each epoch (across all batches)
    t_loss, t_acc = train(train_data_loader, model, cost, optimizer)
    v_loss, v_acc = evaluate(valid_data_loader, model, cost)

    #Append the results for each epoch to our lists so we can graph them later
    train_loss.append(t_loss)
    train_accuracy.append(t_acc)
    valid_loss.append(v_loss)
    valid_accuracy.append(v_acc)

    #Keep track of the lowest loss found, and store the model parameters for that
    #run in a PyTorch file (.pt extension)
    if v_loss < best_valid_loss:
        best_valid_loss = v_loss
        torch.save(model.state_dict(), "ReviewClassifier.pt")

    #See results as the model trains- if results are not improving over time
    #kill the run and check your code
    print("Epoch:", epoch)
    print("train_loss:", round(t_loss, 3), "train_acc:", round(t_acc, 3))
    print("valid_loss:", round(v_loss, 3), "valid_acc:", round(v_acc, 3))

#Use MatPlotLib to chart the loss for the training and validation datasets over the 10 epochs
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1)
ax.plot(train_loss, label="train loss")
ax.plot(valid_loss, label="valid loss")
ax.set_xlabel("epoch")
ax.set_ylabel("loss")
ax.set_xticks(range(10))
ax.legend()
ax.grid()

#Use MatPlotLib to chart the accuracy for the training and validation datasets over the 10 epochs
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1)
ax.plot(train_accuracy, label="train accuracy")
ax.plot(valid_accuracy, label="valid accuracy")
ax.set_xlabel("epoch")
ax.set_ylabel("loss")
ax.set_xticks(range(10))
ax.legend()
ax.grid()

"""# Test model

"""

#Load the model parameters set that achieved the lowest loss from our saved file
#Run the evaluate function on the *test* dataset to see how the model performs on
#unseen samples. Note we don't pass in the optimizer anymore (not changing model params)
model.load_state_dict(torch.load("ReviewClassifier.pt"))
test_loss, test_acc = evaluate(test_data_loader, model, cost)

print("test_loss:", round(test_loss, 3), "test_acc:", round(test_acc, 3))